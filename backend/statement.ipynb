{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall principle of the project\n",
    "\n",
    "The primary objective of the project is to automate the process of fact-checking by implementing a structured approach. This involves first extracting fact statements from natural language text in the form of triples consisting of a Subject, Relation, and Object. The next step is to match the extracted relation to predefined properties, such as associating \"is the capital of\" with a property like dbpedia-owl:capital. Once the relation is identified, the system looks up the subject and object in DBpedia by converting their names into URIs—for example, resolving \"Paris\" to http://dbpedia.org/resource/Paris. Using these URIs, a SPARQL query is executed against the DBpedia knowledge base to verify whether the Subject–Relation–Object triple exists in the data. If the triple is found, the statement is deemed verified. In cases where a direct match cannot be established, the system may employ indirect methods or corrections, such as determining if the subject's \"actual birthplace\" aligns with the stated location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of the new words and their definition : \n",
    "\n",
    "- URI (Uniform Resource Identifier)\n",
    "A URI is a string used to uniquely identify a resource on the internet. In DBpedia, every entity (for example, “Barack Obama” or “Paris”) is assigned a unique URI, such as http://dbpedia.org/resource/Barack_Obama. This URI allows different datasets and applications to talk about the same concept without ambiguity.\n",
    "\n",
    "- DBpedia\n",
    "DBpedia is a large, open-source knowledge base that is built from structured information extracted from Wikipedia. It provides data in the form of RDF triples and makes it possible to query this information using SPARQL. Actually  its an online database describing tens of thousands of people, places, organizations ...\n",
    "\n",
    "- Triplet \n",
    "In our project, a triplet is a statement with three parts: Subject, Predicate (or Relation), Object. For example:\n",
    "Subject: “Barack Obama”\n",
    "Predicate/Relation: “was born in”\n",
    "Object: “Hawaii”\n",
    "\n",
    "- SPARQL\n",
    "SPARQL is a query language designed for querying and manipulating RDF (Resource Description Framework) data. It is used to retrieve or update information stored in a knowledge base like DBpedia. \n",
    "\n",
    "- S, R, and O\n",
    "\n",
    "S stands for Subject. This is the entity about which we are making a statement (e.g., “Barack Obama”).\n",
    "R stands for Relation or Predicate. This is the property or relationship that links subject and object (e.g., “was born in”).\n",
    "O stands for Object. This is the target entity or concept connected to the subject (e.g., “Hawaii”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install spacy requests fuzzywuzzy python-Levenshtein flask flask-cors SPARQLWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necessary libraries. We need requests for HTTP requests to the DBpedia API, fuzzywuzzy for approximate string matching, spacy for NLP tasks such as tokenization and part of speech analysis and SPARQLWrapper to query DBpedia through SPARQL. \n",
    "\n",
    "we also useET from xml.etree.ElementTree because the DBpedia Lookup service returns data in XML format instead of JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\calar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\calar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\calar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\calar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "\n",
    "# For string matching\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# spacyfor NLP\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# SPARQLWrapper for the queries ofDBpedia\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# For XML parsing to handle DBpedia XML response\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "print(\"All imports done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy model loaded.\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"spaCy model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we define a dictionary that maps certain text-based relations (like “was born in”) to the corresponding DBpedia ontology properties (like dbpedia.org/ontology/birthPlace). This helps us to match the queries to actual DBpedia properties. We also have a fuzzy match threshold to decide how close the relation text must be to a known key. We store constants for SPARQL endpoints and the DBpedia Lookup URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation map and endpoints configured.\n"
     ]
    }
   ],
   "source": [
    "RELATION_MAP = {\n",
    "    \"was born in\": [\n",
    "        \"http://dbpedia.org/ontology/birthPlace\",\n",
    "        \"http://dbpedia.org/property/birthPlace\"\n",
    "    ],\n",
    "    \"is the capital of\": [\n",
    "        \"http://dbpedia.org/ontology/capital\",\n",
    "        \"http://dbpedia.org/property/capital\"\n",
    "    ],\n",
    "    \"died in\": [\n",
    "        \"http://dbpedia.org/ontology/deathPlace\",\n",
    "        \"http://dbpedia.org/property/deathPlace\"\n",
    "    ],\n",
    "    \"is located in\": [\n",
    "        \"http://dbpedia.org/ontology/location\",\n",
    "        \"http://dbpedia.org/property/location\"\n",
    "    ],\n",
    "    \"founded\": [\n",
    "        \"http://dbpedia.org/ontology/founder\",\n",
    "        \"http://dbpedia.org/property/founder\"\n",
    "    ],\n",
    "    \"was founded by\": [\n",
    "        \"http://dbpedia.org/ontology/founder\",\n",
    "        \"http://dbpedia.org/property/founder\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "FUZZY_MATCH_THRESHOLD = 70\n",
    "\n",
    "DBPEDIA_SPARQL_ENDPOINT = \"https://dbpedia.org/sparql\"\n",
    "DBPEDIA_LOOKUP_URL = \"https://lookup.dbpedia.org/api/search/KeywordSearch\"\n",
    "\n",
    "print(\"Relation map and endpoints configured.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RELATION_MAP is a A manual dictionary that links a plain English phrase to the actual DBpedia ontology and property URIs\n",
    "FUZZY_MATCH_THRESHOLD = 70 => We need at least 70% similarity to consider the relation a match.\n",
    "DBPEDIA_SPARQL_ENDPOINT => The main SPARQL endpoint of DBpedia.\n",
    "DBPEDIA_LOOKUP_URL => API for DBpedia Lookup service to find the correct resource URIs for an entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we define a helper function with spacy to show how a sentence is tokenized and what each tokens role is (like subject, verb, object). This helps us better understand how spaCy sees the sentence. We run a few example sentences to observe the dependency tree that spaCy outputs, which guide our approach to extracting subject-relation-object triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: spaCy tokens for: 'Paris is the capital of France.'\n",
      "  Paris        nsubj      PROPN  HEAD=is\n",
      "  is           ROOT       AUX    HEAD=is\n",
      "  the          det        DET    HEAD=capital\n",
      "  capital      attr       NOUN   HEAD=is\n",
      "  of           prep       ADP    HEAD=capital\n",
      "  France       pobj       PROPN  HEAD=of\n",
      "  .            punct      PUNCT  HEAD=is\n",
      "--------------------------------------------------\n",
      "DEBUG: spaCy tokens for: 'Elon Musk founded SpaceX.'\n",
      "  Elon         compound   PROPN  HEAD=Musk\n",
      "  Musk         nsubj      PROPN  HEAD=founded\n",
      "  founded      ROOT       VERB   HEAD=founded\n",
      "  SpaceX.      dobj       NOUN   HEAD=founded\n",
      "--------------------------------------------------\n",
      "DEBUG: spaCy tokens for: 'Apple was founded by Steve Jobs.'\n",
      "  Apple        nsubjpass  PROPN  HEAD=founded\n",
      "  was          auxpass    AUX    HEAD=founded\n",
      "  founded      ROOT       VERB   HEAD=founded\n",
      "  by           agent      ADP    HEAD=founded\n",
      "  Steve        compound   PROPN  HEAD=Jobs\n",
      "  Jobs         pobj       PROPN  HEAD=by\n",
      "  .            punct      PUNCT  HEAD=founded\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def debug_spacy_parse(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    print(f\"DEBUG: spaCy tokens for: '{sentence}'\")\n",
    "    for token in doc:\n",
    "        print(f\"  {token.text:<12} {token.dep_:<10} {token.pos_:<6} HEAD={token.head.text}\")\n",
    "\n",
    "test_sentences = [\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"Elon Musk founded SpaceX.\",\n",
    "    \"Apple was founded by Steve Jobs.\"\n",
    "]\n",
    "for s in test_sentences:\n",
    "    debug_spacy_parse(s)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy processes the textt, for token in doc we iterate over each token to print out dependency labels (like nsubj, ROOT, etc.).\n",
    "We test with simple sentences to confirm how spaCy labels them, for instance verifying that Paris is recognized as nsubj."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function has an approach to triple extraction => the subject is taken as the first noun chunk, the object is taken as the last noun chunk, and the words between them (verbs, auxiliaries, prepositions) are joined to form the relation text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG EXTRACT: S='Barack Obama'  R='was born in'  O='Hawaii'\n",
      "Result => ('Barack Obama', 'was born in', 'Hawaii')\n",
      "--------------------------------------------------\n",
      "DEBUG EXTRACT: S='Paris'  R='is of'  O='France'\n",
      "Result => ('Paris', 'is of', 'France')\n",
      "--------------------------------------------------\n",
      "DEBUG EXTRACT: S='Elon Musk'  R='founded'  O='Spacex.'\n",
      "Result => ('Elon Musk', 'founded', 'Spacex.')\n",
      "--------------------------------------------------\n",
      "DEBUG EXTRACT: S='Apple'  R='was founded by'  O='Steve Jobs'\n",
      "Result => ('Apple', 'was founded by', 'Steve Jobs')\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def extract_triplet_spacy(sentence):\n",
    "    \"\"\"\n",
    "     - subject = first noun chunk\n",
    "     - object = last noun chunk\n",
    "     - relation = verbs in between\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    noun_chunks = list(doc.noun_chunks)\n",
    "    if len(noun_chunks) < 2:\n",
    "        print(\"DEBUG: Less than 2 noun chunks, can't extract triple properly.\")\n",
    "        return None\n",
    "    \n",
    "    subject_chunk = noun_chunks[0]\n",
    "    object_chunk = noun_chunks[-1]\n",
    "    subject_text = subject_chunk.text.strip()\n",
    "    object_text = object_chunk.text.strip()\n",
    "    \n",
    "    start_idx = subject_chunk.end\n",
    "    end_idx = object_chunk.start\n",
    "    \n",
    "    if start_idx >= end_idx:\n",
    "        print(\"DEBUG: Overlapping subject/object chunk, can't parse relation.\")\n",
    "        return None\n",
    "    \n",
    "    relation_tokens = []\n",
    "    for token in doc[start_idx:end_idx]:\n",
    "        if token.pos_ in [\"VERB\", \"AUX\", \"ADP\", \"PART\"]:\n",
    "            relation_tokens.append(token.text)\n",
    "\n",
    "    relation_text = \" \".join(relation_tokens).strip()\n",
    "    \n",
    "    if not relation_text:\n",
    "        print(f\"DEBUG: Extracted an empty relation between {subject_text} and {object_text}.\")\n",
    "    \n",
    "    def normalize_entity_name(x):\n",
    "        return \" \".join(t.capitalize() for t in x.split())\n",
    "    \n",
    "    subject_text = normalize_entity_name(subject_text)\n",
    "    object_text = normalize_entity_name(object_text)\n",
    "    \n",
    "    print(f\"DEBUG EXTRACT: S='{subject_text}'  R='{relation_text}'  O='{object_text}'\")\n",
    "    return (subject_text, relation_text, object_text)\n",
    "\n",
    "demo = [\n",
    "    \"Barack Obama was born in Hawaii.\",\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"Elon Musk founded SpaceX.\",\n",
    "    \"Apple was founded by Steve Jobs.\"\n",
    "]\n",
    "for d in demo:\n",
    "    res = extract_triplet_spacy(d)\n",
    "    print(\"Result =>\", res)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the relation, we only consider tokens in between that are typical “relation indicators” like verbs, auxiliary verbs, prepositions, or particles.\n",
    "This method illustrate the concept of triple extraction for fact checking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that takes the extracted relation text (for ex “was born in”) and tries to match it to one of the known keys in RELATION_MAP. We use fuzz.ratio from fuzzywuzzy to handle slight differences like “was founded by” or “is the capital of”. If the match is above a threshold, we accept it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 'founded' => founded\n",
      "Testing 'was founded by' => was founded by\n",
      "Testing 'is the capital of' => is the capital of\n",
      "Testing 'was born in' => was born in\n"
     ]
    }
   ],
   "source": [
    "def find_best_relation_match(relation_text, relation_map=RELATION_MAP):\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "    for candidate in relation_map.keys():\n",
    "        score = fuzz.ratio(relation_text.lower(), candidate.lower())\n",
    "        if score > best_score and score >= FUZZY_MATCH_THRESHOLD:\n",
    "            best_score = score\n",
    "            best_match = candidate\n",
    "    return best_match\n",
    "\n",
    "raw_relation = \"founded\"\n",
    "print(\"Testing 'founded' =>\", find_best_relation_match(raw_relation))\n",
    "raw_relation = \"was founded by\"\n",
    "print(\"Testing 'was founded by' =>\", find_best_relation_match(raw_relation))\n",
    "raw_relation = \"is the capital of\"\n",
    "print(\"Testing 'is the capital of' =>\", find_best_relation_match(raw_relation))\n",
    "raw_relation = \"was born in\"\n",
    "print(\"Testing 'was born in' =>\", find_best_relation_match(raw_relation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following cell addresses how we query DBpedia to turn a text like “Barack Obama” into a valid DBpedia URI (http://dbpedia.org/resource/Barack_Obama). Because the DBpedia Lookup API return XML, we parse the XML with xml.etree.ElementTree to get the <URI> tags. We also demonstrate a quick test with examples like “Barack Obama” and “Paris.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Barack Obama\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Barack%20Obama&MaxHits=3, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Barack Obama</Label><URI>http://dbpedia.org/resource/Barack_Obama</URI><Description>Barack Hussein Obama II ( (); born August 4, 19\n",
      "URIs => ['http://dbpedia.org/resource/Barack_Obama', 'http://dbpedia.org/resource/List_of_federal_judges_appointed_by_Barack_Obama', 'http://dbpedia.org/resource/Family_of_Barack_Obama']\n",
      "--------------------------------------------------\n",
      "Entity: Paris\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Paris&MaxHits=3, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Paris</Label><URI>http://dbpedia.org/resource/Paris</URI><Description>Paris (French pronunciation: ​[paʁi] ()) is the capital and m\n",
      "URIs => ['http://dbpedia.org/resource/Paris', 'http://dbpedia.org/resource/Central_European_Time', 'http://dbpedia.org/resource/Paris_Saint-Germain_F.C.']\n",
      "--------------------------------------------------\n",
      "Entity: Elvis Presley\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Elvis%20Presley&MaxHits=3, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Elvis Presley</Label><URI>http://dbpedia.org/resource/Elvis_Presley</URI><Description>Elvis Aaron Presley (January 8, 1935 – August\n",
      "URIs => ['http://dbpedia.org/resource/Elvis_Presley', 'http://dbpedia.org/resource/Elvis_Presley_on_film_and_television', 'http://dbpedia.org/resource/Elvis_Presley_singles_discography']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#DBPEDIA LOOKUP + XML PARSING\n",
    "from urllib.parse import quote\n",
    "\n",
    "def lookup_dbpedia_entities(entity_text, max_results=3):\n",
    "    #Query the DBpedia Lookup API for a text\n",
    "    #Returns a list of candidate URIs (parsed from XML)\n",
    "    url = f\"{DBPEDIA_LOOKUP_URL}?QueryString={quote(entity_text)}&MaxHits={max_results}\"\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",  # DBpedia Lookup often ignores this\n",
    "        \"User-Agent\": \"Mozilla/5.0 (compatible; MyFactChecker/1.0)\"\n",
    "    }\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=10)\n",
    "        print(f\"DEBUG: Lookup URL = {resp.url}, status_code={resp.status_code}\")\n",
    "        \n",
    "        # Print first 200 chars for debugging\n",
    "        text_sample = resp.text[:200]\n",
    "        print(\"DEBUG: Response content (first 200 chars):\", text_sample.replace(\"\\n\",\"\\\\n\"))\n",
    "        \n",
    "        resp.raise_for_status()  # Raise if not 200\n",
    "        # The returned data is actually XMLso we parse\n",
    "        root = ET.fromstring(resp.text)\n",
    "        uris = []\n",
    "        for i, result_tag in enumerate(root.findall('Result')):\n",
    "            if i >= max_results:\n",
    "                break\n",
    "            uri_tag = result_tag.find('URI')\n",
    "            if uri_tag is not None:\n",
    "                uris.append(uri_tag.text)\n",
    "        return uris\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"DBpedia Lookup error: {e}\")\n",
    "        return []\n",
    "    except ET.ParseError as pe:\n",
    "        # Error parsing XML\n",
    "        print(f\"DBpedia Lookup XML parse error: {pe}\")\n",
    "        return []\n",
    "\n",
    "test_entities = [\"Barack Obama\", \"Paris\", \"Elvis Presley\"]\n",
    "for ent in test_entities:\n",
    "    print(f\"Entity: {ent}\")\n",
    "    res = lookup_dbpedia_entities(ent)\n",
    "    print(\"URIs =>\", res)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use quote(entity_text) to ensure the users text is URL-encoded.\n",
    "We keep only up to max_results URIs so we don’t get too many.\n",
    "The reason we use XML parsing  is because the DBpedia Lookup service  return XML by default, not JSON. In many cases, DBpedia ignores the Accept: application/json header, so we must parse the XML format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we performs a SPARQL ASK query to see if a specific triple exists in DBpedia. If subject_uri, property_uri, and object_uri appear together in DBpedia, we get True; otherwise, False. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG SPARQL ASK: http://dbpedia.org/resource/Paris http://dbpedia.org/ontology/capital http://dbpedia.org/resource/France  => False\n"
     ]
    }
   ],
   "source": [
    "#  SPARQL ASK\n",
    "def check_triple_in_dbpedia(subject_uri, property_uri, object_uri):\n",
    "    \"\"\"\n",
    "    Query DBpedia via SPARQL to see if subject_uri property_uri object_uri exists.\n",
    "    Return True/False.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    ASK WHERE {{\n",
    "       <{subject_uri}> <{property_uri}> <{object_uri}> .\n",
    "    }}\n",
    "    \"\"\"\n",
    "    sparql = SPARQLWrapper(DBPEDIA_SPARQL_ENDPOINT)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        return results[\"boolean\"]  \n",
    "    except Exception as e:\n",
    "        print(f\"SPARQL error: {e}\")\n",
    "        return False\n",
    "\n",
    "test_subj = \"http://dbpedia.org/resource/Paris\"\n",
    "test_prop = \"http://dbpedia.org/ontology/capital\"\n",
    "test_obj = \"http://dbpedia.org/resource/France\"\n",
    "\n",
    "res = check_triple_in_dbpedia(test_subj, test_prop, test_obj)\n",
    "print(\"DEBUG SPARQL ASK:\", test_subj, test_prop, test_obj, \" =>\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBpedia  encodes “Paris is the capital of France” differently like as France dbo:capital Paris maybe and not Paris dbo:capital France.  in that orientation, ASK returns false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, check_triple_in_dbpedia_flexible is an other approach where we retrieve the objects for (subject_uri, property_uri, ?val) and compare their labels to the users requested label. It uses fuzzy matching to see if any of these ?val labels are close to user_object_label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_triple_in_dbpedia_flexible(subject_uri, property_uri, user_object_label):\n",
    "    query = f\"\"\"\n",
    "    SELECT ?val WHERE {{\n",
    "      <{subject_uri}> <{property_uri}> ?val .\n",
    "    }}\n",
    "    \"\"\"\n",
    "    results = query_dbpedia(query)  \n",
    "    \n",
    "    if not results:\n",
    "        return False\n",
    "\n",
    "    # 2) Pour chaque valeur ?val, tenter de récupérer un label\n",
    "    for binding in results:\n",
    "        val_uri = binding[\"val\"][\"value\"]\n",
    "        \n",
    "        # on reconstruit un label en enlevant le préfixe DBpedia et en remplaçant _\n",
    "        if val_uri.startswith(\"http://dbpedia.org/resource/\"):\n",
    "            short_name = val_uri.replace(\"http://dbpedia.org/resource/\", \"\")\n",
    "            short_name_clean = short_name.replace(\"_\", \" \").replace(\",\", \"\")\n",
    "            # => \"Los Angeles California\"\n",
    "            \n",
    "            # Fuzzy match\n",
    "            score = fuzz.ratio(short_name_clean.lower(), user_object_label.lower())\n",
    "            if score > 80:\n",
    "                return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This cell is the central part of the system**\n",
    "\n",
    "A list of transitive location properties that help us see if child_uri is indirectly part of a bigger region (for ex Pretoria → South Africa).\n",
    "Also a helper function to fetch object URIs for a given property, do BFS to see if something is included in a parent region, and to look up the “correct” object if the direct triple is not found.\n",
    "The check_fact_statement function orchestrates the entire pipeline: extracting a triple from the user statement, fuzzy matching the relation, looking up DBpedia URIs, SPARQL-checking them, and if that fails, attempting a “correction” (e.g. we discover Elon Musk was actually born in “Pretoria,” then see if Pretoria is located in “South Africa”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSITIVE_PROPS_LOCATION = [\n",
    "    \"http://dbpedia.org/ontology/location\",\n",
    "    \"http://dbpedia.org/property/location\",\n",
    "    \"http://dbpedia.org/ontology/isPartOf\",\n",
    "    \"http://dbpedia.org/ontology/country\",\n",
    "    \"http://dbpedia.org/ontology/region\",\n",
    "    \"http://dbpedia.org/ontology/city\",\n",
    "]\n",
    "\n",
    "def get_objects_for_property(subject_uri, property_uri):\n",
    "    #Fait un SELECT ?o WHERE { <subject_uri> <property_uri> ?o } et retourne la liste des URIs objets correspondants.\n",
    "    sparql = SPARQLWrapper(DBPEDIA_SPARQL_ENDPOINT)\n",
    "    query = f\"\"\"\n",
    "    SELECT ?o WHERE {{\n",
    "      <{subject_uri}> <{property_uri}> ?o .\n",
    "    }}\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = []\n",
    "    try:\n",
    "        res = sparql.query().convert()\n",
    "        for b in res[\"results\"][\"bindings\"]:\n",
    "            results.append(b[\"o\"][\"value\"])\n",
    "    except Exception as e:\n",
    "        print(\"SPARQL error in get_objects_for_property:\", e)\n",
    "    return results\n",
    "\n",
    "def is_included_in(child_uri, parent_label, max_depth=2):\n",
    "    #Parcours BFS pour déterminer si child_uri (ex. Louvre) \n",
    "    #est inclus (directement ou indirectement) dans parent_label (ex. \"Paris\").\n",
    "    #On limite la profondeur (max_depth) pour éviter un trop grand nombre de requêtes.\n",
    "    #S'il trouve un label proche de parent_label, renvoie True.\n",
    "    from collections import deque\n",
    "    visited = set()\n",
    "    queue = deque([(child_uri, 0)])\n",
    "    \n",
    "    while queue:\n",
    "        current_uri, depth = queue.popleft()\n",
    "        if current_uri in visited:\n",
    "            continue\n",
    "        visited.add(current_uri)\n",
    "\n",
    "        # Récupérer un label local (ex. \"Louvre\" depuis http://dbpedia.org/resource/Louvre)\n",
    "        label_current = uri_to_label(current_uri)\n",
    "        \n",
    "        # Fuzzy match avec parent_label (ex. \"Paris\")\n",
    "        score = fuzz.ratio(label_current.lower(), parent_label.lower())\n",
    "        if score >= 80:\n",
    "            return True  # on a trouvé une correspondance satisfaisante\n",
    "\n",
    "        # Si on peut encore descendre, on explore les propriétés transitives\n",
    "        if depth < max_depth:\n",
    "            for p in TRANSITIVE_PROPS_LOCATION:\n",
    "                voisins = get_objects_for_property(current_uri, p)\n",
    "                for v in voisins:\n",
    "                    if v not in visited:\n",
    "                        queue.append((v, depth + 1))\n",
    "    \n",
    "    return False\n",
    "\n",
    "def try_correction_lookup(subject_uris, matched_relation):\n",
    "    #Pour certaines relations (ex: was born in, died in, located in...), \n",
    "    #on récupère la/les \"vraies\" valeurs DBpedia.\n",
    "    #On renvoie un dict avec : - \"text\": un message textuel (ex. \"Mona Lisa is actually located at the Louvre.\")\n",
    "    # et - \"object_uris\": la liste des URIs candidates (ex. [http://dbpedia.org/resource/Louvre])\n",
    "    \n",
    "    correction_map = {\n",
    "        \"was born in\": \"http://dbpedia.org/ontology/birthPlace\",\n",
    "        \"died in\": \"http://dbpedia.org/ontology/deathPlace\",\n",
    "        \"is located in\": \"http://dbpedia.org/ontology/location\", \n",
    "    }\n",
    "    if matched_relation not in correction_map:\n",
    "        return None\n",
    "    \n",
    "    prop = correction_map[matched_relation]\n",
    "    s_uri = subject_uris[0]  # on tente le premier URI du sujet\n",
    "    values = fetch_property_values(s_uri, prop, limit=5)\n",
    "    if not values:\n",
    "        return None\n",
    "    \n",
    "    short_labels = [uri_to_label(v) for v in values]\n",
    "    # On assemble un petit texte explicatif\n",
    "    txt = (f\"{uri_to_label(s_uri)}’s actual {matched_relation} is: \"\n",
    "           + \", \".join(short_labels) + \".\")\n",
    "    \n",
    "    return {\n",
    "        \"text\": txt,\n",
    "        \"object_uris\": values\n",
    "    }\n",
    "\n",
    "def check_fact_statement(statement):\n",
    "    print(\"\\n==== FACT CHECK STATEMENT ====\")\n",
    "    print(f\"User statement: '{statement}'\")\n",
    "    \n",
    "    # 1) Extraction\n",
    "    extracted = extract_triplet_spacy(statement)\n",
    "    if not extracted:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": \"Could not extract (subject, relation, object) from statement.\",\n",
    "            \"statement\": statement\n",
    "        }\n",
    "    \n",
    "    subject_text, raw_relation, object_text = extracted\n",
    "    \n",
    "    # 2) Relation fuzzy match\n",
    "    matched_relation_key = find_best_relation_match(raw_relation, RELATION_MAP)\n",
    "    print(f\"DEBUG: raw_relation='{raw_relation}', matched_relation_key='{matched_relation_key}'\")\n",
    "    \n",
    "    if not matched_relation_key:\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"found\": False,\n",
    "            \"message\": f\"Relation '{raw_relation}' not recognized in RELATION_MAP. I cannot confirm or deny this statement.\",\n",
    "            \"extracted_triplet\": extracted\n",
    "        }\n",
    "    \n",
    "    possible_props = RELATION_MAP[matched_relation_key]\n",
    "    \n",
    "    # 3) Lookup DBpedia URIs (sujet / objet)\n",
    "    max_to_check = 5\n",
    "    subj_candidates = lookup_dbpedia_entities(subject_text, max_results=15)[:max_to_check]\n",
    "    obj_candidates  = lookup_dbpedia_entities(object_text, max_results=15)[:max_to_check]\n",
    "    \n",
    "    print(\"DEBUG: Subject candidates =>\", subj_candidates)\n",
    "    print(\"DEBUG: Object candidates  =>\", obj_candidates)\n",
    "    \n",
    "    if not subj_candidates:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"found\": False,\n",
    "            \"message\": f\"No DBpedia resource found for subject '{subject_text}'\",\n",
    "            \"extracted_triplet\": extracted\n",
    "        }\n",
    "    if not obj_candidates:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"found\": False,\n",
    "            \"message\": f\"No DBpedia resource found for object '{object_text}'\",\n",
    "            \"extracted_triplet\": extracted\n",
    "        }\n",
    "    \n",
    "    # 4) Vérif direct SPARQL\n",
    "    found_any = False\n",
    "    matched_triples = []\n",
    "    \n",
    "    for s_uri in subj_candidates:\n",
    "        for prop_uri in possible_props:\n",
    "            for o_uri in obj_candidates:\n",
    "                if check_triple_in_dbpedia(s_uri, prop_uri, o_uri):\n",
    "                    found_any = True\n",
    "                    matched_triples.append((s_uri, prop_uri, \"Fuzzy matched -> \" + object_text))\n",
    "                    break\n",
    "            if found_any:\n",
    "                break\n",
    "        if found_any:\n",
    "            break\n",
    "    \n",
    "    if found_any:\n",
    "        # On a trouvé un triplet exact => la déclaration est vraie directement\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"found\": True,\n",
    "            \"message\": \"Statement verified. One or more matching triples found in DBpedia.\",\n",
    "            \"extracted_triplet\": extracted,\n",
    "            \"matched_triples\": matched_triples\n",
    "        }\n",
    "    else:\n",
    "        # Pas trouvé => on tente la \"correction\" + inclusion indirecte\n",
    "        correction_info = try_correction_lookup(subj_candidates, matched_relation_key)\n",
    "        \n",
    "        if correction_info:\n",
    "            # On regarde si la/les valeurs renvoyées sont incluses dans object_text\n",
    "            real_uris = correction_info.get(\"object_uris\", [])\n",
    "            # On fait un BFS \n",
    "            for real_uri in real_uris:\n",
    "                if is_included_in(real_uri, object_text, max_depth=2):\n",
    "                    # => Indirectement vrai\n",
    "                    return {\n",
    "                        \"success\": True,\n",
    "                        \"found\": True,\n",
    "                        \"message\": (\n",
    "                            f\"Indirectly true: {uri_to_label(subj_candidates[0])} \"\n",
    "                            f\"{matched_relation_key} {uri_to_label(real_uri)}, \"\n",
    "                            f\"which is itself included in '{object_text}'.\"\n",
    "                        ),\n",
    "                        \"extracted_triplet\": extracted\n",
    "                    }\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"found\": False,\n",
    "                \"message\": (\n",
    "                    \"No matching triple found in DBpedia for the recognized relation. \"\n",
    "                    f\"However, I found a different fact: {correction_info['text']}\"\n",
    "                ),\n",
    "                \"extracted_triplet\": extracted\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"found\": False,\n",
    "                \"message\": \"No matching triple found in DBpedia for the recognized relation.\",\n",
    "                \"extracted_triplet\": extracted\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def try_correction_lookup(subject_uris, matched_relation):\n",
    "    correction_map = {\n",
    "        \"was born in\": \"http://dbpedia.org/ontology/birthPlace\",\n",
    "        \"died in\": \"http://dbpedia.org/ontology/deathPlace\",\n",
    "        \"is located in\": \"http://dbpedia.org/ontology/location\", \n",
    "    }\n",
    "    if matched_relation not in correction_map:\n",
    "        return None\n",
    "    \n",
    "    prop = correction_map[matched_relation]\n",
    "    s_uri = subject_uris[0]  \n",
    "    values = fetch_property_values(s_uri, prop, limit=5)\n",
    "    if not values:\n",
    "        return None\n",
    "    \n",
    "    short_labels = [uri_to_label(v) for v in values]\n",
    "    txt = (f\"{uri_to_label(s_uri)}’s actual {matched_relation} is: \"\n",
    "           + \", \".join(short_labels) + \".\")\n",
    "    \n",
    "    return {\n",
    "        \"text\": txt,\n",
    "        \"object_uris\": values\n",
    "    }\n",
    "\n",
    "\n",
    "def fetch_property_values(subject_uri, property_uri, limit=5):\n",
    "    #SPARQL query: SELECT ?value WHERE { <subject_uri> <property_uri> ?value } LIMIT n\n",
    "    #Return list of object URIs\n",
    "    from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "    query = f\"\"\"\n",
    "    SELECT ?val WHERE {{\n",
    "      <{subject_uri}> <{property_uri}> ?val .\n",
    "    }} LIMIT {limit}\n",
    "    \"\"\"\n",
    "    endpoint = SPARQLWrapper(DBPEDIA_SPARQL_ENDPOINT)\n",
    "    endpoint.setQuery(query)\n",
    "    endpoint.setReturnFormat(JSON)\n",
    "    values = []\n",
    "    try:\n",
    "        results = endpoint.query().convert()\n",
    "        for b in results[\"results\"][\"bindings\"]:\n",
    "            values.append(b[\"val\"][\"value\"])\n",
    "    except Exception as e:\n",
    "        print(\"SPARQL error in fetch_property_values:\", e)\n",
    "    return values\n",
    "\n",
    "def uri_to_label(uri):\n",
    "    if uri.startswith(\"http://dbpedia.org/resource/\"):\n",
    "        name_part = uri.replace(\"http://dbpedia.org/resource/\", \"\")\n",
    "        return name_part.replace(\"_\", \" \")\n",
    "    else:\n",
    "        return uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a list of sample statements to test and then we call check_fact_statement on each. The debug output shows how the system extracts a triple, matches the relation, searches DBpedia, and either confirms or corrects the statement.\n",
    "\n",
    "Each statement triggers the entire pipeline => triple extraction, DBpedia lookup, SPARQL ask, correction, and possible indirect checks.\n",
    "The results show “true,” “false,” or an “indirectly true” scenario, plus any other fallback messages about discovered facts (like Elvis Presley actually died in Memphis, not Paris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== FACT CHECK STATEMENT ====\n",
      "User statement: 'Barack Obama was born in Hawaii'\n",
      "DEBUG EXTRACT: S='Barack Obama'  R='was born in'  O='Hawaii'\n",
      "DEBUG: raw_relation='was born in', matched_relation_key='was born in'\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Barack%20Obama&MaxHits=15, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Barack Obama</Label><URI>http://dbpedia.org/resource/Barack_Obama</URI><Description>Barack Hussein Obama II ( (); born August 4, 19\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Hawaii&MaxHits=15, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Hawaii</Label><URI>http://dbpedia.org/resource/Hawaii</URI><Description>Hawaiʻi ( () hə-WY-ee; Hawaiian: Hawaiʻi [həˈvɐjʔi]) is a s\n",
      "DEBUG: Subject candidates => ['http://dbpedia.org/resource/Barack_Obama', 'http://dbpedia.org/resource/List_of_federal_judges_appointed_by_Barack_Obama', 'http://dbpedia.org/resource/Family_of_Barack_Obama', 'http://dbpedia.org/resource/Presidency_of_Barack_Obama', 'http://dbpedia.org/resource/Barack_Obama_citizenship_conspiracy_theories']\n",
      "DEBUG: Object candidates  => ['http://dbpedia.org/resource/Hawaii', 'http://dbpedia.org/resource/List_of_state_highways_in_Hawaii', 'http://dbpedia.org/resource/Territory_of_Hawaii', 'http://dbpedia.org/resource/Honolulu', 'http://dbpedia.org/resource/Hilo,_Hawaii']\n",
      "RESULT => {\n",
      "  \"success\": true,\n",
      "  \"found\": true,\n",
      "  \"message\": \"Statement verified. One or more matching triples found in DBpedia.\",\n",
      "  \"extracted_triplet\": [\n",
      "    \"Barack Obama\",\n",
      "    \"was born in\",\n",
      "    \"Hawaii\"\n",
      "  ],\n",
      "  \"matched_triples\": [\n",
      "    [\n",
      "      \"http://dbpedia.org/resource/Barack_Obama\",\n",
      "      \"http://dbpedia.org/ontology/birthPlace\",\n",
      "      \"Fuzzy matched -> Hawaii\"\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "======================================================================\n",
      "\n",
      "==== FACT CHECK STATEMENT ====\n",
      "User statement: 'Paris is the capital of France'\n",
      "DEBUG EXTRACT: S='Paris'  R='is of'  O='France'\n",
      "DEBUG: raw_relation='is of', matched_relation_key='None'\n",
      "RESULT => {\n",
      "  \"success\": true,\n",
      "  \"found\": false,\n",
      "  \"message\": \"Relation 'is of' not recognized in RELATION_MAP. I cannot confirm or deny this statement.\",\n",
      "  \"extracted_triplet\": [\n",
      "    \"Paris\",\n",
      "    \"is of\",\n",
      "    \"France\"\n",
      "  ]\n",
      "}\n",
      "======================================================================\n",
      "\n",
      "==== FACT CHECK STATEMENT ====\n",
      "User statement: 'Elon Musk was born in South Africa'\n",
      "DEBUG EXTRACT: S='Elon Musk'  R='was born in'  O='South Africa'\n",
      "DEBUG: raw_relation='was born in', matched_relation_key='was born in'\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Elon%20Musk&MaxHits=15, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Elon Musk</Label><URI>http://dbpedia.org/resource/Elon_Musk</URI><Description>Elon Reeve Musk  (; born June 28, 1971) is an enginee\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=South%20Africa&MaxHits=15, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>South Africa</Label><URI>http://dbpedia.org/resource/South_Africa</URI><Description>South Africa, officially the Republic of South \n",
      "DEBUG: Subject candidates => ['http://dbpedia.org/resource/Elon_Musk', 'http://dbpedia.org/resource/Acquisition_of_Twitter_by_Elon_Musk', 'http://dbpedia.org/resource/Elon_Musk:_Tesla,_SpaceX,_and_the_Quest_for_a_Fantastic_Future', 'http://dbpedia.org/resource/Views_of_Elon_Musk', 'http://dbpedia.org/resource/Yellowknife']\n",
      "DEBUG: Object candidates  => ['http://dbpedia.org/resource/South_Africa', 'http://dbpedia.org/resource/South_African_Standard_Time', 'http://dbpedia.org/resource/Union_of_South_Africa', 'http://dbpedia.org/resource/Second_Boer_War', 'http://dbpedia.org/resource/Local_municipality_(South_Africa)']\n",
      "RESULT => {\n",
      "  \"success\": true,\n",
      "  \"found\": true,\n",
      "  \"message\": \"Indirectly true: Elon Musk was born in Pretoria, which is itself included in 'South Africa'.\",\n",
      "  \"extracted_triplet\": [\n",
      "    \"Elon Musk\",\n",
      "    \"was born in\",\n",
      "    \"South Africa\"\n",
      "  ]\n",
      "}\n",
      "======================================================================\n",
      "\n",
      "==== FACT CHECK STATEMENT ====\n",
      "User statement: 'Elvis Presley died in Paris'\n",
      "DEBUG EXTRACT: S='Elvis Presley'  R='died in'  O='Paris'\n",
      "DEBUG: raw_relation='died in', matched_relation_key='died in'\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Elvis%20Presley&MaxHits=15, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Elvis Presley</Label><URI>http://dbpedia.org/resource/Elvis_Presley</URI><Description>Elvis Aaron Presley (January 8, 1935 – August\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Paris&MaxHits=15, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Paris</Label><URI>http://dbpedia.org/resource/Paris</URI><Description>Paris (French pronunciation: ​[paʁi] ()) is the capital and m\n",
      "DEBUG: Subject candidates => ['http://dbpedia.org/resource/Elvis_Presley', 'http://dbpedia.org/resource/Elvis_Presley_on_film_and_television', 'http://dbpedia.org/resource/Elvis_Presley_singles_discography', 'http://dbpedia.org/resource/Elvis_Presley_(album)', 'http://dbpedia.org/resource/List_of_songs_recorded_by_Elvis_Presley']\n",
      "DEBUG: Object candidates  => ['http://dbpedia.org/resource/Paris', 'http://dbpedia.org/resource/Central_European_Time', 'http://dbpedia.org/resource/Paris_Saint-Germain_F.C.', 'http://dbpedia.org/resource/Paris_FC', 'http://dbpedia.org/resource/University_of_Paris']\n",
      "RESULT => {\n",
      "  \"success\": true,\n",
      "  \"found\": false,\n",
      "  \"message\": \"No matching triple found in DBpedia for the recognized relation. However, I found a different fact: Elvis Presley\\u2019s actual died in is: Memphis, Tennessee.\",\n",
      "  \"extracted_triplet\": [\n",
      "    \"Elvis Presley\",\n",
      "    \"died in\",\n",
      "    \"Paris\"\n",
      "  ]\n",
      "}\n",
      "======================================================================\n",
      "\n",
      "==== FACT CHECK STATEMENT ====\n",
      "User statement: 'Albert Einstein was born in Ulm'\n",
      "DEBUG EXTRACT: S='Albert Einstein'  R='was born in'  O='Ulm'\n",
      "DEBUG: raw_relation='was born in', matched_relation_key='was born in'\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Albert%20Einstein&MaxHits=15, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Albert Einstein</Label><URI>http://dbpedia.org/resource/Albert_Einstein</URI><Description>Albert Einstein ( EYEN-styne; German: [ˈa\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Ulm&MaxHits=15, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>SSV Ulm 1846</Label><URI>http://dbpedia.org/resource/SSV_Ulm_1846</URI><Description>SSV Ulm 1846 is a German football club from Ulm\n",
      "DEBUG: Subject candidates => ['http://dbpedia.org/resource/Albert_Einstein', 'http://dbpedia.org/resource/Albert_Einstein_College_of_Medicine', 'http://dbpedia.org/resource/Albert_Einstein_Medal', 'http://dbpedia.org/resource/Albert_Einstein_World_Award_of_Science', 'http://dbpedia.org/resource/Einstein_family']\n",
      "DEBUG: Object candidates  => ['http://dbpedia.org/resource/SSV_Ulm_1846', 'http://dbpedia.org/resource/Ulm', 'http://dbpedia.org/resource/Ratiopharm_Ulm', 'http://dbpedia.org/resource/New_Ulm,_Minnesota', 'http://dbpedia.org/resource/Neu-Ulm']\n",
      "RESULT => {\n",
      "  \"success\": true,\n",
      "  \"found\": true,\n",
      "  \"message\": \"Statement verified. One or more matching triples found in DBpedia.\",\n",
      "  \"extracted_triplet\": [\n",
      "    \"Albert Einstein\",\n",
      "    \"was born in\",\n",
      "    \"Ulm\"\n",
      "  ],\n",
      "  \"matched_triples\": [\n",
      "    [\n",
      "      \"http://dbpedia.org/resource/Albert_Einstein\",\n",
      "      \"http://dbpedia.org/ontology/birthPlace\",\n",
      "      \"Fuzzy matched -> Ulm\"\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "======================================================================\n",
      "\n",
      "==== FACT CHECK STATEMENT ====\n",
      "User statement: 'Albert Einstein was born in Berlin'\n",
      "DEBUG EXTRACT: S='Albert Einstein'  R='was born in'  O='Berlin'\n",
      "DEBUG: raw_relation='was born in', matched_relation_key='was born in'\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Albert%20Einstein&MaxHits=15, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Albert Einstein</Label><URI>http://dbpedia.org/resource/Albert_Einstein</URI><Description>Albert Einstein ( EYEN-styne; German: [ˈa\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Berlin&MaxHits=15, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Berlin</Label><URI>http://dbpedia.org/resource/Berlin</URI><Description>Berlin (; German: [bɛʁˈliːn] ()) is the capital and largest\n",
      "DEBUG: Subject candidates => ['http://dbpedia.org/resource/Albert_Einstein', 'http://dbpedia.org/resource/Albert_Einstein_College_of_Medicine', 'http://dbpedia.org/resource/Albert_Einstein_Medal', 'http://dbpedia.org/resource/Albert_Einstein_World_Award_of_Science', 'http://dbpedia.org/resource/Einstein_family']\n",
      "DEBUG: Object candidates  => ['http://dbpedia.org/resource/Berlin', 'http://dbpedia.org/resource/Tennis_Borussia_Berlin', 'http://dbpedia.org/resource/1._FC_Union_Berlin', 'http://dbpedia.org/resource/West_Berlin', 'http://dbpedia.org/resource/Hertha_BSC']\n",
      "RESULT => {\n",
      "  \"success\": true,\n",
      "  \"found\": false,\n",
      "  \"message\": \"No matching triple found in DBpedia for the recognized relation. However, I found a different fact: Albert Einstein\\u2019s actual was born in is: Ulm, German Empire, Kingdom of W\\u00fcrttemberg.\",\n",
      "  \"extracted_triplet\": [\n",
      "    \"Albert Einstein\",\n",
      "    \"was born in\",\n",
      "    \"Berlin\"\n",
      "  ]\n",
      "}\n",
      "======================================================================\n",
      "\n",
      "==== FACT CHECK STATEMENT ====\n",
      "User statement: 'Donald Trump was born in Paris'\n",
      "DEBUG EXTRACT: S='Donald Trump'  R='was born in'  O='Paris'\n",
      "DEBUG: raw_relation='was born in', matched_relation_key='was born in'\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Donald%20Trump&MaxHits=15, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Donald Trump</Label><URI>http://dbpedia.org/resource/Donald_Trump</URI><Description>Donald John Trump (born June 14, 1946) is the 4\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Paris&MaxHits=15, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Paris</Label><URI>http://dbpedia.org/resource/Paris</URI><Description>Paris (French pronunciation: ​[paʁi] ()) is the capital and m\n",
      "DEBUG: Subject candidates => ['http://dbpedia.org/resource/Donald_Trump', 'http://dbpedia.org/resource/List_of_federal_judges_appointed_by_Donald_Trump', 'http://dbpedia.org/resource/Impeachment_of_Donald_Trump', 'http://dbpedia.org/resource/Donald_Trump_Jr.', 'http://dbpedia.org/resource/Presidency_of_Donald_Trump']\n",
      "DEBUG: Object candidates  => ['http://dbpedia.org/resource/Paris', 'http://dbpedia.org/resource/Central_European_Time', 'http://dbpedia.org/resource/Paris_Saint-Germain_F.C.', 'http://dbpedia.org/resource/Paris_FC', 'http://dbpedia.org/resource/University_of_Paris']\n",
      "RESULT => {\n",
      "  \"success\": true,\n",
      "  \"found\": false,\n",
      "  \"message\": \"No matching triple found in DBpedia for the recognized relation. However, I found a different fact: Donald Trump\\u2019s actual was born in is: Queens.\",\n",
      "  \"extracted_triplet\": [\n",
      "    \"Donald Trump\",\n",
      "    \"was born in\",\n",
      "    \"Paris\"\n",
      "  ]\n",
      "}\n",
      "======================================================================\n",
      "\n",
      "==== FACT CHECK STATEMENT ====\n",
      "User statement: 'Michael Jackson died in Los Angeles'\n",
      "DEBUG EXTRACT: S='Michael Jackson'  R='died in'  O='Los Angeles'\n",
      "DEBUG: raw_relation='died in', matched_relation_key='died in'\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Michael%20Jackson&MaxHits=15, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Michael Jackson</Label><URI>http://dbpedia.org/resource/Michael_Jackson</URI><Description>Michael Joseph Jackson (August 29, 1958 –\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Los%20Angeles&MaxHits=15, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Los Angeles</Label><URI>http://dbpedia.org/resource/Los_Angeles</URI><Description>Los Angeles ( (); Spanish: Los Ángeles; Spanish f\n",
      "DEBUG: Subject candidates => ['http://dbpedia.org/resource/Michael_Jackson', 'http://dbpedia.org/resource/Trial_of_Michael_Jackson', 'http://dbpedia.org/resource/Invincible_(Michael_Jackson_album)', 'http://dbpedia.org/resource/Michael_Jackson_(disambiguation)', 'http://dbpedia.org/resource/Death_of_Michael_Jackson']\n",
      "DEBUG: Object candidates  => ['http://dbpedia.org/resource/Los_Angeles', 'http://dbpedia.org/resource/Los_Angeles_Kings', 'http://dbpedia.org/resource/University_of_California,_Los_Angeles', 'http://dbpedia.org/resource/Los_Angeles_Rams', 'http://dbpedia.org/resource/Los_Angeles_County,_California']\n",
      "RESULT => {\n",
      "  \"success\": true,\n",
      "  \"found\": false,\n",
      "  \"message\": \"No matching triple found in DBpedia for the recognized relation. However, I found a different fact: Michael Jackson\\u2019s actual died in is: Los Angeles, California.\",\n",
      "  \"extracted_triplet\": [\n",
      "    \"Michael Jackson\",\n",
      "    \"died in\",\n",
      "    \"Los Angeles\"\n",
      "  ]\n",
      "}\n",
      "======================================================================\n",
      "\n",
      "==== FACT CHECK STATEMENT ====\n",
      "User statement: 'Michael Jackson died in 2009 in the city of Los Angeles'\n",
      "DEBUG EXTRACT: S='Michael Jackson'  R='died in in of'  O='Los Angeles'\n",
      "DEBUG: raw_relation='died in in of', matched_relation_key='died in'\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Michael%20Jackson&MaxHits=15, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Michael Jackson</Label><URI>http://dbpedia.org/resource/Michael_Jackson</URI><Description>Michael Joseph Jackson (August 29, 1958 –\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Los%20Angeles&MaxHits=15, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Los Angeles</Label><URI>http://dbpedia.org/resource/Los_Angeles</URI><Description>Los Angeles ( (); Spanish: Los Ángeles; Spanish f\n",
      "DEBUG: Subject candidates => ['http://dbpedia.org/resource/Michael_Jackson', 'http://dbpedia.org/resource/Trial_of_Michael_Jackson', 'http://dbpedia.org/resource/Invincible_(Michael_Jackson_album)', 'http://dbpedia.org/resource/Michael_Jackson_(disambiguation)', 'http://dbpedia.org/resource/Death_of_Michael_Jackson']\n",
      "DEBUG: Object candidates  => ['http://dbpedia.org/resource/Los_Angeles', 'http://dbpedia.org/resource/Los_Angeles_Kings', 'http://dbpedia.org/resource/University_of_California,_Los_Angeles', 'http://dbpedia.org/resource/Los_Angeles_Rams', 'http://dbpedia.org/resource/Los_Angeles_County,_California']\n",
      "RESULT => {\n",
      "  \"success\": true,\n",
      "  \"found\": false,\n",
      "  \"message\": \"No matching triple found in DBpedia for the recognized relation. However, I found a different fact: Michael Jackson\\u2019s actual died in is: Los Angeles, California.\",\n",
      "  \"extracted_triplet\": [\n",
      "    \"Michael Jackson\",\n",
      "    \"died in in of\",\n",
      "    \"Los Angeles\"\n",
      "  ]\n",
      "}\n",
      "======================================================================\n",
      "\n",
      "==== FACT CHECK STATEMENT ====\n",
      "User statement: 'Apple was founded by Steve Jobs'\n",
      "DEBUG EXTRACT: S='Apple'  R='was founded by'  O='Steve Jobs'\n",
      "DEBUG: raw_relation='was founded by', matched_relation_key='was founded by'\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Apple&MaxHits=15, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Apple II</Label><URI>http://dbpedia.org/resource/Apple_II</URI><Description>The Apple II (stylized as apple ][) is an 8-bit home co\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Steve%20Jobs&MaxHits=15, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Steve Jobs</Label><URI>http://dbpedia.org/resource/Steve_Jobs</URI><Description>Steven Paul Jobs (; February 24, 1955 – October 5, \n",
      "DEBUG: Subject candidates => ['http://dbpedia.org/resource/Apple_II', 'http://dbpedia.org/resource/Apple_Inc.', 'http://dbpedia.org/resource/IOS', 'http://dbpedia.org/resource/MacOS', 'http://dbpedia.org/resource/Apple_Records']\n",
      "DEBUG: Object candidates  => ['http://dbpedia.org/resource/Steve_Jobs', 'http://dbpedia.org/resource/Steve_Jobs_(book)', 'http://dbpedia.org/resource/List_of_artistic_depictions_of_Steve_Jobs', 'http://dbpedia.org/resource/Daniel_Lyons', 'http://dbpedia.org/resource/Steve_Jobs:_The_Lost_Interview']\n",
      "RESULT => {\n",
      "  \"success\": true,\n",
      "  \"found\": false,\n",
      "  \"message\": \"No matching triple found in DBpedia for the recognized relation.\",\n",
      "  \"extracted_triplet\": [\n",
      "    \"Apple\",\n",
      "    \"was founded by\",\n",
      "    \"Steve Jobs\"\n",
      "  ]\n",
      "}\n",
      "======================================================================\n",
      "\n",
      "==== FACT CHECK STATEMENT ====\n",
      "User statement: 'Elon Musk founded SpaceX'\n",
      "DEBUG: Less than 2 noun chunks, can't extract triple properly.\n",
      "RESULT => {\n",
      "  \"success\": false,\n",
      "  \"error\": \"Could not extract (subject, relation, object) from statement.\",\n",
      "  \"statement\": \"Elon Musk founded SpaceX\"\n",
      "}\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "test_statements = [\n",
    "    \"Barack Obama was born in Hawaii\",\n",
    "    \"Paris is the capital of France\",\n",
    "    \"Elon Musk was born in South Africa\",\n",
    "    \"Elvis Presley died in Paris\",\n",
    "    \"Albert Einstein was born in Ulm\",\n",
    "    \"Albert Einstein was born in Berlin\",\n",
    "    \"Donald Trump was born in Paris\",\n",
    "    \"Michael Jackson died in Los Angeles\",\n",
    "    \"Michael Jackson died in 2009 in the city of Los Angeles\",\n",
    "    \"Apple was founded by Steve Jobs\",\n",
    "    \"Elon Musk founded SpaceX\"\n",
    "]\n",
    "\n",
    "for st in test_statements:\n",
    "    result = check_fact_statement(st)\n",
    "    print(\"RESULT =>\", json.dumps(result, indent=2))\n",
    "    print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://dbpedia.org/resource/Cabinet_of_Barack_Obama\n",
      "http://dbpedia.org/resource/A_Singular_Woman:_The_Untold_Story_of_Barack_Obama's_Mother\n",
      "http://dbpedia.org/resource/BARACK_OBAMA\n",
      "http://dbpedia.org/resource/Presidency_of_Barack_Obama\n",
      "http://dbpedia.org/resource/President_Barack_Obama\n",
      "http://dbpedia.org/resource/President_Elect_Barack_Obama\n",
      "http://dbpedia.org/resource/Electoral_history_of_Barack_Obama\n",
      "http://dbpedia.org/resource/Energy_policy_of_the_Barack_Obama_administration\n",
      "http://dbpedia.org/resource/List_of_awards_and_honors_received_by_Barack_Obama\n",
      "http://dbpedia.org/resource/List_of_bills_sponsored_by_Barack_Obama_in_the_United_States_Senate\n"
     ]
    }
   ],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "query = \"\"\"\n",
    "SELECT ?s WHERE {\n",
    "    ?s rdfs:label ?label .\n",
    "    FILTER (lang(?label) = 'en').\n",
    "    FILTER (CONTAINS(lcase(?label), lcase(\"Barack Obama\"))).\n",
    "} LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "sparql.setQuery(query)\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    print(result[\"s\"][\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install flask flask-cors requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le serveur Flask est démarré sur http://127.0.0.1:5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import io\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from contextlib import redirect_stdout\n",
    "from threading import Thread\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "SAMPLE_QUESTIONS = [\n",
    "    \"Barack Obama was born in Hawaii\",\n",
    "    \"Elvis Presley died in Memphis\",\n",
    "    \"Albert Einstein was born in Ulm\",\n",
    "    \"Donald Trump was born in Queens\",\n",
    "    \"Paris is the capital of France\",\n",
    "    \"Michael Jackson died in Los Angeles\",\n",
    "    \"Celine Dion was born in Canada\",\n",
    "    \"Elvis Presley died in Paris\",          \n",
    "    \"Donald Trump was born in Paris\",       \n",
    "    \"Albert Einstein was born in Berlin\",   \n",
    "]\n",
    "\n",
    "@app.route(\"/get_suggestions\", methods=[\"GET\"])\n",
    "def get_suggestions():\n",
    "    suggestions = random.sample(SAMPLE_QUESTIONS, 2)\n",
    "    return jsonify({\n",
    "        \"suggestions\": suggestions\n",
    "    })\n",
    "\n",
    "@app.route(\"/ask\", methods=[\"POST\"])\n",
    "def ask(): \n",
    "    #exécute check_fact_statement, et renvoie le résultat + les logs debug.\n",
    "    data = request.get_json(force=True)\n",
    "    user_question = data.get(\"question\", \"\").strip()\n",
    "    \n",
    "    if not user_question:\n",
    "        return jsonify({\"error\": \"No question provided.\"}), 400\n",
    "    \n",
    "    print(\"=== DEBUG FLASK: Question reçue:\", user_question)  # Log de debug\n",
    "    \n",
    "    f_debug = io.StringIO()\n",
    "    with redirect_stdout(f_debug):\n",
    "        print(\"=== DEBUG FLASK: Début de l'exécution de check_fact_statement\") \n",
    "        result = check_fact_statement(user_question)\n",
    "        print(\"=== DEBUG FLASK: Résultat brut de check_fact_statement:\", result)  \n",
    "        print(\"RESULT =>\", json.dumps(result, indent=2))\n",
    "        print(\"=== DEBUG FLASK: Fin de l'exécution\") \n",
    "    \n",
    "    debug_output = f_debug.getvalue()  # tout ce qui a été \"printé\"\n",
    "    print(\"=== DEBUG FLASK: Contenu capturé dans debug_output:\", debug_output)  \n",
    "    \n",
    "    response = {\n",
    "        \"debug\": debug_output,\n",
    "        \"result\": result\n",
    "    }\n",
    "    print(\"=== DEBUG FLASK: Réponse finale envoyée:\", json.dumps(response, indent=2)) \n",
    "    \n",
    "    return jsonify(response)\n",
    "    \n",
    "def run_flask():\n",
    "    app.run(debug=True, use_reloader=False)  \n",
    "\n",
    "#  thread si on est dans un notebook\n",
    "flask_thread = Thread(target=run_flask)\n",
    "flask_thread.daemon = True  # Le thread s'arrêtera quand le notebook sera arrêté\n",
    "flask_thread.start()\n",
    "\n",
    "print(\"Le serveur Flask est démarré sur http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import requests\\n\\nBASE_URL = \"http://127.0.0.1:5000\"\\n\\n# 1) Récupérer 2 suggestions\\nresp = requests.get(f\"{BASE_URL}/get_suggestions\")\\nif resp.status_code == 200:\\n    print(\"Suggestions =>\", resp.json())\\nelse:\\n    print(\"Error:\", resp.text)\\n\\n# 2) Envoyer une question\\nquestion_to_ask = \"Barack Obama was born in Hawaii\"\\nresp2 = requests.post(f\"{BASE_URL}/ask\", json={\"question\": question_to_ask})\\nif resp2.status_code == 200:\\n    data = resp2.json()\\n    print(\"\\n=== DEBUG LOGS ===\")\\n    print(data[\"debug\"])\\n    print(\"=== RESULT ===\")\\n    print(data[\"result\"])\\nelse:\\n    print(\"Error:\", resp2.text)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [15/Feb/2025 15:25:10] \"GET /get_suggestions HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [15/Feb/2025 15:25:12] \"OPTIONS /ask HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUG FLASK: Question reçue: Barack Obama was born in Hawaii\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [15/Feb/2025 15:25:15] \"POST /ask HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUG FLASK: Contenu capturé dans debug_output: === DEBUG FLASK: Début de l'exécution de check_fact_statement\n",
      "\n",
      "==== FACT CHECK STATEMENT ====\n",
      "User statement: 'Barack Obama was born in Hawaii'\n",
      "DEBUG EXTRACT: S='Barack Obama'  R='was born in'  O='Hawaii'\n",
      "DEBUG: raw_relation='was born in', matched_relation_key='was born in'\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Barack%20Obama&MaxHits=15, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Barack Obama</Label><URI>http://dbpedia.org/resource/Barack_Obama</URI><Description>Barack Hussein Obama II ( (); born August 4, 19\n",
      "DEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Hawaii&MaxHits=15, status_code=200\n",
      "DEBUG: Response content (first 200 chars): <?xml version=\"1.0\" encoding=\"UTF-8\"?><ArrayOfResults><Result><Label>Hawaii</Label><URI>http://dbpedia.org/resource/Hawaii</URI><Description>Hawaiʻi ( () hə-WY-ee; Hawaiian: Hawaiʻi [həˈvɐjʔi]) is a s\n",
      "DEBUG: Subject candidates => ['http://dbpedia.org/resource/Barack_Obama', 'http://dbpedia.org/resource/List_of_federal_judges_appointed_by_Barack_Obama', 'http://dbpedia.org/resource/Family_of_Barack_Obama', 'http://dbpedia.org/resource/Presidency_of_Barack_Obama', 'http://dbpedia.org/resource/Barack_Obama_citizenship_conspiracy_theories']\n",
      "DEBUG: Object candidates  => ['http://dbpedia.org/resource/Hawaii', 'http://dbpedia.org/resource/List_of_state_highways_in_Hawaii', 'http://dbpedia.org/resource/Territory_of_Hawaii', 'http://dbpedia.org/resource/Honolulu', 'http://dbpedia.org/resource/Hilo,_Hawaii']\n",
      "=== DEBUG FLASK: Résultat brut de check_fact_statement: {'success': True, 'found': True, 'message': 'Statement verified. One or more matching triples found in DBpedia.', 'extracted_triplet': ('Barack Obama', 'was born in', 'Hawaii'), 'matched_triples': [('http://dbpedia.org/resource/Barack_Obama', 'http://dbpedia.org/ontology/birthPlace', 'Fuzzy matched -> Hawaii')]}\n",
      "RESULT => {\n",
      "  \"success\": true,\n",
      "  \"found\": true,\n",
      "  \"message\": \"Statement verified. One or more matching triples found in DBpedia.\",\n",
      "  \"extracted_triplet\": [\n",
      "    \"Barack Obama\",\n",
      "    \"was born in\",\n",
      "    \"Hawaii\"\n",
      "  ],\n",
      "  \"matched_triples\": [\n",
      "    [\n",
      "      \"http://dbpedia.org/resource/Barack_Obama\",\n",
      "      \"http://dbpedia.org/ontology/birthPlace\",\n",
      "      \"Fuzzy matched -> Hawaii\"\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "=== DEBUG FLASK: Fin de l'exécution\n",
      "\n",
      "=== DEBUG FLASK: Réponse finale envoyée: {\n",
      "  \"debug\": \"=== DEBUG FLASK: D\\u00e9but de l'ex\\u00e9cution de check_fact_statement\\n\\n==== FACT CHECK STATEMENT ====\\nUser statement: 'Barack Obama was born in Hawaii'\\nDEBUG EXTRACT: S='Barack Obama'  R='was born in'  O='Hawaii'\\nDEBUG: raw_relation='was born in', matched_relation_key='was born in'\\nDEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Barack%20Obama&MaxHits=15, status_code=200\\nDEBUG: Response content (first 200 chars): <?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?><ArrayOfResults><Result><Label>Barack Obama</Label><URI>http://dbpedia.org/resource/Barack_Obama</URI><Description>Barack Hussein Obama II ( (); born August 4, 19\\nDEBUG: Lookup URL = https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString=Hawaii&MaxHits=15, status_code=200\\nDEBUG: Response content (first 200 chars): <?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?><ArrayOfResults><Result><Label>Hawaii</Label><URI>http://dbpedia.org/resource/Hawaii</URI><Description>Hawai\\u02bbi ( () h\\u0259-WY-ee; Hawaiian: Hawai\\u02bbi [h\\u0259\\u02c8v\\u0250j\\u0294i]) is a s\\nDEBUG: Subject candidates => ['http://dbpedia.org/resource/Barack_Obama', 'http://dbpedia.org/resource/List_of_federal_judges_appointed_by_Barack_Obama', 'http://dbpedia.org/resource/Family_of_Barack_Obama', 'http://dbpedia.org/resource/Presidency_of_Barack_Obama', 'http://dbpedia.org/resource/Barack_Obama_citizenship_conspiracy_theories']\\nDEBUG: Object candidates  => ['http://dbpedia.org/resource/Hawaii', 'http://dbpedia.org/resource/List_of_state_highways_in_Hawaii', 'http://dbpedia.org/resource/Territory_of_Hawaii', 'http://dbpedia.org/resource/Honolulu', 'http://dbpedia.org/resource/Hilo,_Hawaii']\\n=== DEBUG FLASK: R\\u00e9sultat brut de check_fact_statement: {'success': True, 'found': True, 'message': 'Statement verified. One or more matching triples found in DBpedia.', 'extracted_triplet': ('Barack Obama', 'was born in', 'Hawaii'), 'matched_triples': [('http://dbpedia.org/resource/Barack_Obama', 'http://dbpedia.org/ontology/birthPlace', 'Fuzzy matched -> Hawaii')]}\\nRESULT => {\\n  \\\"success\\\": true,\\n  \\\"found\\\": true,\\n  \\\"message\\\": \\\"Statement verified. One or more matching triples found in DBpedia.\\\",\\n  \\\"extracted_triplet\\\": [\\n    \\\"Barack Obama\\\",\\n    \\\"was born in\\\",\\n    \\\"Hawaii\\\"\\n  ],\\n  \\\"matched_triples\\\": [\\n    [\\n      \\\"http://dbpedia.org/resource/Barack_Obama\\\",\\n      \\\"http://dbpedia.org/ontology/birthPlace\\\",\\n      \\\"Fuzzy matched -> Hawaii\\\"\\n    ]\\n  ]\\n}\\n=== DEBUG FLASK: Fin de l'ex\\u00e9cution\\n\",\n",
      "  \"result\": {\n",
      "    \"success\": true,\n",
      "    \"found\": true,\n",
      "    \"message\": \"Statement verified. One or more matching triples found in DBpedia.\",\n",
      "    \"extracted_triplet\": [\n",
      "      \"Barack Obama\",\n",
      "      \"was born in\",\n",
      "      \"Hawaii\"\n",
      "    ],\n",
      "    \"matched_triples\": [\n",
      "      [\n",
      "        \"http://dbpedia.org/resource/Barack_Obama\",\n",
      "        \"http://dbpedia.org/ontology/birthPlace\",\n",
      "        \"Fuzzy matched -> Hawaii\"\n",
      "      ]\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [15/Feb/2025 15:25:21] \"GET /get_suggestions HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "'''import requests\n",
    "\n",
    "BASE_URL = \"http://127.0.0.1:5000\"\n",
    "\n",
    "# 1) Récupérer 2 suggestions\n",
    "resp = requests.get(f\"{BASE_URL}/get_suggestions\")\n",
    "if resp.status_code == 200:\n",
    "    print(\"Suggestions =>\", resp.json())\n",
    "else:\n",
    "    print(\"Error:\", resp.text)\n",
    "\n",
    "# 2) Envoyer une question\n",
    "question_to_ask = \"Barack Obama was born in Hawaii\"\n",
    "resp2 = requests.post(f\"{BASE_URL}/ask\", json={\"question\": question_to_ask})\n",
    "if resp2.status_code == 200:\n",
    "    data = resp2.json()\n",
    "    print(\"\\n=== DEBUG LOGS ===\")\n",
    "    print(data[\"debug\"])\n",
    "    print(\"=== RESULT ===\")\n",
    "    print(data[\"result\"])\n",
    "else:\n",
    "    print(\"Error:\", resp2.text)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "With all these cells, we build a pipeline that:\n",
    "\n",
    "- Extracts a candidate triple from a user statement.\n",
    "- Matches the relation with a known DBpedia property.\n",
    "- Finds the best URI for subject/object using DBpedia Lookup.\n",
    "- Uses SPARQL queries to check if the triple is in DBpedia.\n",
    "- If not found, attempts an indirect or “correction” approach to handle partial truths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, our fact-checking can verify statements by extracting triplets, matching them to a known set of relations, and querying DBpedia to see if those relations appear in the knowledge base. The current project is still quite , because of a lack of time to develop it. It handles only a few predefined types of relations (e.g. “was born in,” “died in,” “founded”) and relies on fairly simple methods of extracting triples from text. The usage of DBpedias Lookup API can also fail or produce uncertain results for more ambiguous queries, and we only implement a small part of DBpedia’s ontology (not all relations or classes).\n",
    "\n",
    "- What is missing and what could be improved:\n",
    "\n",
    " The system now depends on a “naive chunking” approach where the subject is assumed to be the first noun phrase and the object the last noun phrase. A more sophisticated NLP pipeline could better distinguish multiple subjects, objects, and verb phrases.\n",
    "\n",
    " We only still handle a limited set of known relations like “was born in,” “died in,” etc. We would need to integrate a richer set of relations  to handle more complex statements.\n",
    "\n",
    " Using SPARQL queries at scale might become slow if we process large volumes of statements or rely on more advanced reasoning steps. We would want to consider local caching or more optimized infrastructure solutions.\n",
    "\n",
    "- How to extend toward a fully automated system:\n",
    "\n",
    "Integrate advanced NLP: Use a more comprehensive relation-extraction framework that identifies both standard and custom relations, as well as handles complex sentence structures (passive voices, coordination, or nested clauses).\n",
    "\n",
    "Expand the ontology: Link to a broader range of DBpedia or Wikidata properties to cover more statements. We could also maintain our own local knowledge graph that is frequently updated and refined.\n",
    "\n",
    "Better reasoning: Develop a reasoning engine that can chain multiple facts, handle potential contradictions, and rank the most likely matches. This could involve both symbolic logic (for deducing transitive or hierarchical facts) and machine learning (for computing similarity or confidence scores).\n",
    "\n",
    "\n",
    "Overall, the existing code demonstrates the core logic extracting triplets, matching relations, querying a knowledge base, and returning whether the statement is confirmed, partially confirmed, or not confirmed at all. Much work remains to handle more diverse, ambiguous, or indirect statements, and to scale up robustly for real-world, automated fact-checking scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
